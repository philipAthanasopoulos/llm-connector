"use strict";var $=Object.create;var _=Object.defineProperty;var G=Object.getOwnPropertyDescriptor;var L=Object.getOwnPropertyNames;var N=Object.getPrototypeOf,z=Object.prototype.hasOwnProperty;var K=(n,e,t,s)=>{if(e&&typeof e=="object"||typeof e=="function")for(let o of L(e))!z.call(n,o)&&o!==t&&_(n,o,{get:()=>e[o],enumerable:!(s=G(e,o))||s.enumerable});return n};var J=(n,e,t)=>(t=n!=null?$(N(n)):{},K(e||!n||!n.__esModule?_(t,"default",{value:n,enumerable:!0}):t,n));Object.defineProperties(exports,{__esModule:{value:!0},[Symbol.toStringTag]:{value:"Module"}});const g=require("react"),m=require("react-chatbotify"),q={autoConfig:!0},H=(n,e)=>{const t=g.useCallback(s=>{const r=n()[s.data.nextPath];e(r)},[n,e]);m.useOnRcbEvent(m.RcbEvent.CHANGE_PATH,t)},Y=(n,e)=>{const{outputTypeRef:t}=n,{toggleTextAreaDisabled:s,toggleIsBotTyping:o,focusTextArea:r,injectMessage:a,simulateStreamMessage:i,getIsChatBotVisible:c}=e,l=g.useCallback(d=>{var u;const h=d.data.block;h.llmConnector&&(d.preventDefault(),d.type==="rcb-pre-process-block"&&((u=h.llmConnector)!=null&&u.initialMessage&&(t.current==="full"?a(n.initialMessageRef.current):i(n.initialMessageRef.current)),o(!1),s(!1),setTimeout(()=>{c()&&r()})))},[o,s,r,c]);m.useOnRcbEvent(m.RcbEvent.PRE_PROCESS_BLOCK,l),m.useOnRcbEvent(m.RcbEvent.POST_PROCESS_BLOCK,l)},V=async function*(n,e){for await(const t of n)for(const s of t)yield s,await new Promise(o=>setTimeout(o,e))},Q=async function*(n,e){for await(const t of n)yield t,await new Promise(s=>setTimeout(s,e))},X=async function*(n,e,t){e==="character"?yield*V(n,t):yield*Q(n,t)},Z=async function*(n,e){for await(const t of n)e(t),yield t},ee=async(n,e,t,s={})=>{var R,M;if(!e.providerRef.current)return;const{speakAudio:o,toggleIsBotTyping:r,toggleTextAreaDisabled:a,focusTextArea:i,injectMessage:c,streamMessage:l,endStreamMessage:d,getIsChatBotVisible:h}=t,u=e.providerRef.current.sendMessages(n),b=e.outputTypeRef.current,y=e.outputSpeedRef.current;if(b==="full"){let p="";for await(const f of u){if((R=s.signal)!=null&&R.aborted)break;p+=f}r(!1),c(p),setTimeout(()=>{a(!1),h()&&i()})}else{const p=X(Z(u,o),b,y);let f="",S=!1;for await(const E of p){if((M=s.signal)!=null&&M.aborted)break;S||(r(!1),S=!0),f+=E,l(f)}d(),setTimeout(()=>{a(!1),h()&&i()})}},te=500,se=(n,e)=>{const{messagesRef:t,outputTypeRef:s,onUserMessageRef:o,onKeyDownRef:r,errorMessageRef:a}=n,{injectMessage:i,simulateStreamMessage:c,toggleTextAreaDisabled:l,toggleIsBotTyping:d,goToPath:h,focusTextArea:u,getIsChatBotVisible:b}=e,y=g.useRef(null),R=g.useCallback(M=>{if(!n.providerRef.current)return;const p=M.data.message,f=p.sender.toUpperCase();p.tags=p.tags??[],p.tags.push(`rcb-llm-connector-plugin:${f}`),f==="USER"&&(d(!0),l(!0),setTimeout(async()=>{var T;if(o.current){const P=await o.current(p);if(P)return(T=y.current)==null||T.abort(),y.current=null,h(P)}const S=n.historySizeRef.current,E=t.current,v=S?[...E.slice(-(S-1)),p]:[p],C=new AbortController;y.current=C,ee(v,n,e,{signal:C.signal}).catch(P=>{d(!1),l(!1),setTimeout(()=>{b()&&u()}),console.error("LLM prompt failed",P),s.current==="full"?i(a.current):c(a.current)})},te))},[n,e]);m.useOnRcbEvent(m.RcbEvent.POST_INJECT_MESSAGE,R),m.useOnRcbEvent(m.RcbEvent.STOP_SIMULATE_STREAM_MESSAGE,R),m.useOnRcbEvent(m.RcbEvent.STOP_STREAM_MESSAGE,R),g.useEffect(()=>{const M=async p=>{var f;if(r.current){const S=await r.current(p);S&&((f=y.current)==null||f.abort(),y.current=null,h(S))}};return window.addEventListener("keydown",M),()=>window.removeEventListener("keydown",M)},[])},re=n=>{const e=g.useRef([]),t=g.useRef(null),s=g.useRef("chunk"),o=g.useRef(30),r=g.useRef(0),a=g.useRef(""),i=g.useRef("Unable to get response, please try again."),c=g.useRef(null),l=g.useRef(null),{getFlow:d}=m.useFlow(),{speakAudio:h}=m.useAudio(),{messages:u,injectMessage:b,simulateStreamMessage:y,streamMessage:R,endStreamMessage:M}=m.useMessages(),{goToPath:p}=m.usePaths(),{toggleTextAreaDisabled:f,focusTextArea:S}=m.useTextArea(),{toggleIsBotTyping:E,getIsChatBotVisible:v}=m.useChatWindow(),C={...q,...n??{}};g.useEffect(()=>{e.current=u},[u]),H(d,w=>{var x,A,k,B,U,I,j,D,F,W;t.current=((x=w.llmConnector)==null?void 0:x.provider)??null,s.current=((A=w.llmConnector)==null?void 0:A.outputType)??"chunk",o.current=((k=w.llmConnector)==null?void 0:k.outputSpeed)??30,r.current=((B=w.llmConnector)==null?void 0:B.historySize)??0,a.current=((U=w.llmConnector)==null?void 0:U.initialMessage)??"",i.current=((I=w.llmConnector)==null?void 0:I.errorMessage)??"Unable to get response, please try again.",c.current=((D=(j=w.llmConnector)==null?void 0:j.stopConditions)==null?void 0:D.onUserMessage)??null,l.current=((W=(F=w.llmConnector)==null?void 0:F.stopConditions)==null?void 0:W.onKeyDown)??null});const T={providerRef:t,messagesRef:e,outputTypeRef:s,outputSpeedRef:o,historySizeRef:r,initialMessageRef:a,errorMessageRef:i,onUserMessageRef:c,onKeyDownRef:l},P={speakAudio:h,injectMessage:b,simulateStreamMessage:y,streamMessage:R,endStreamMessage:M,toggleTextAreaDisabled:f,toggleIsBotTyping:E,focusTextArea:S,goToPath:p,getIsChatBotVisible:v};Y(T,P),se(T,P);const O={name:"@rcb-plugins/llm-connector"};return C!=null&&C.autoConfig&&(O.settings={event:{rcbChangePath:!0,rcbPostInjectMessage:!0,rcbStopSimulateStreamMessage:!0,rcbStopStreamMessage:!0,rcbPreProcessBlock:!0,rcbPostProcessBlock:!0}}),O},oe=n=>()=>re(n);class ne{constructor(e){this.debug=!1,this.roleMap=s=>{switch(s){case"USER":return"user";default:return"model"}},this.constructBodyWithMessages=s=>{let o;return this.messageParser?o=this.messageParser(s):o=s.filter(a=>typeof a.content=="string"&&a.sender.toUpperCase()!=="SYSTEM").map(a=>{const i=this.roleMap(a.sender.toUpperCase()),c=a.content;return{role:i,parts:[{text:c}]}}),this.systemMessage&&(o=[{role:"user",parts:[{text:this.systemMessage}]},...o]),{contents:o,...this.body}},this.handleStreamResponse=async function*(s){var a,i,c,l,d;const o=new TextDecoder("utf-8");let r="";for(;;){const{value:h,done:u}=await s.read();if(u)break;r+=o.decode(h,{stream:!0});const b=r.split(`
`);r=b.pop();for(const y of b){const R=y.trim();if(!R.startsWith("data: "))continue;const M=R.slice(6);try{const f=(d=(l=(c=(i=(a=JSON.parse(M).candidates)==null?void 0:a[0])==null?void 0:i.content)==null?void 0:c.parts)==null?void 0:l[0])==null?void 0:d.text;f&&(yield f)}catch(p){console.error("SSE JSON parse error:",M,p)}}}},this.method=e.method??"POST",this.body=e.body??{},this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.debug=e.debug??!1,this.headers={"Content-Type":"application/json",Accept:this.responseFormat==="stream"?"text/event-stream":"application/json",...e.headers};const t=e.baseUrl??"https://generativelanguage.googleapis.com/v1beta";if(e.mode==="direct")this.endpoint=this.responseFormat==="stream"?`${t}/models/${e.model}:streamGenerateContent?alt=sse&key=${e.apiKey||""}`:`${t}/models/${e.model}:generateContent?key=${e.apiKey||""}`;else if(e.mode==="proxy")this.endpoint=`${t}/${e.model}`;else throw Error("Invalid mode specified for Gemini provider ('direct' or 'proxy').")}async*sendMessages(e){var s,o,r,a,i;if(this.debug){const c=this.endpoint.replace(/\?key=([^&]+)/,"?key=[REDACTED]"),l={...this.headers};console.log("[GeminiProvider] Request:",{method:this.method,endpoint:c,headers:l,body:this.constructBodyWithMessages(e)})}const t=await fetch(this.endpoint,{method:this.method,headers:this.headers,body:JSON.stringify(this.constructBodyWithMessages(e))});if(this.debug&&console.log("[GeminiProvider] Response status:",t.status),!t.ok)throw new Error(`Gemini API error ${t.status}: ${await t.text()}`);if(this.responseFormat==="stream"){if(!t.body)throw new Error("Response body is empty – cannot stream");const c=t.body.getReader();for await(const l of this.handleStreamResponse(c))yield l}else{const c=await t.json();this.debug&&console.log("[GeminiProvider] Response body:",c);const l=(i=(a=(r=(o=(s=c.candidates)==null?void 0:s[0])==null?void 0:o.content)==null?void 0:r.parts)==null?void 0:a[0])==null?void 0:i.text;if(typeof l=="string")yield l;else throw new Error("Unexpected response shape – no text candidate")}}}class ae{constructor(e){if(this.debug=!1,this.roleMap=t=>{switch(t){case"USER":return"user";case"SYSTEM":return"system";default:return"assistant"}},this.constructBodyWithMessages=t=>{let s;return this.messageParser?s=this.messageParser(t):s=t.filter(r=>typeof r.content=="string"&&r.sender.toUpperCase()!=="SYSTEM").map(r=>{const a=this.roleMap(r.sender.toUpperCase()),i=r.content;return{role:a,content:i}}),this.systemMessage&&(s=[{role:"system",content:this.systemMessage},...s]),{messages:s,...this.body}},this.handleStreamResponse=async function*(t){var r,a,i;const s=new TextDecoder("utf-8");let o="";for(;;){const{value:c,done:l}=await t.read();if(l)break;o+=s.decode(c,{stream:!0});const d=o.split(/\r?\n/);o=d.pop();for(const h of d){if(!h.startsWith("data: "))continue;const u=h.slice(6).trim();if(u==="[DONE]")return;try{const y=(i=(a=(r=JSON.parse(u).choices)==null?void 0:r[0])==null?void 0:a.delta)==null?void 0:i.content;y&&(yield y)}catch(b){console.error("Stream parse error",b)}}}},this.method=e.method??"POST",this.endpoint=e.baseUrl??"https://api.openai.com/v1/chat/completions",this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.debug=e.debug??!1,this.headers={"Content-Type":"application/json",Accept:this.responseFormat==="stream"?"text/event-stream":"application/json",...e.headers},this.body={model:e.model,stream:this.responseFormat==="stream",...e.body},e.mode==="direct"){this.headers={...this.headers,Authorization:`Bearer ${e.apiKey}`};return}if(e.mode!=="proxy")throw Error("Invalid mode specified for OpenAI provider ('direct' or 'proxy').")}async*sendMessages(e){var s,o,r;if(this.debug){const a={...this.headers};delete a.Authorization,console.log("[OpenaiProvider] Request:",{method:this.method,endpoint:this.endpoint,headers:a,body:this.constructBodyWithMessages(e)})}const t=await fetch(this.endpoint,{method:this.method,headers:this.headers,body:JSON.stringify(this.constructBodyWithMessages(e))});if(this.debug&&console.log("[OpenaiProvider] Response status:",t.status),!t.ok)throw new Error(`Openai API error ${t.status}: ${await t.text()}`);if(this.responseFormat==="stream"){if(!t.body)throw new Error("Response body is empty – cannot stream");const a=t.body.getReader();for await(const i of this.handleStreamResponse(a))yield i}else{const a=await t.json();this.debug&&console.log("[OpenaiProvider] Response body:",a);const i=(r=(o=(s=a.choices)==null?void 0:s[0])==null?void 0:o.message)==null?void 0:r.content;if(typeof i=="string")yield i;else throw new Error("Unexpected response shape – no text candidate")}}}class ie{constructor(e){this.debug=!1,this.roleMap=t=>{switch(t){case"USER":return"user";case"SYSTEM":return"system";default:return"assistant"}},this.constructBodyWithMessages=t=>{let s;return this.messageParser?s=this.messageParser(t):s=t.filter(r=>typeof r.content=="string"&&r.sender.toUpperCase()!=="SYSTEM").map(r=>{const a=this.roleMap(r.sender.toUpperCase()),i=r.content;return{role:a,content:i}}),this.systemMessage&&(s=[{role:"system",content:this.systemMessage},...s]),{messages:s,stream:this.responseFormat==="stream",...this.chatCompletionOptions}},this.model=e.model,this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.engineConfig=e.engineConfig??{},this.chatCompletionOptions=e.chatCompletionOptions??{},this.debug=e.debug??!1,this.createEngine()}async createEngine(){const{CreateMLCEngine:e}=await import("@mlc-ai/web-llm");this.engine=await e(this.model,{...this.engineConfig})}async*sendMessages(e){var s,o,r,a,i,c;this.engine||await this.createEngine(),this.debug&&console.log("[WebLlmProvider] Request:",{model:this.model,systemMessage:this.systemMessage,responseFormat:this.responseFormat,engineConfig:this.engineConfig,chatCompletionOptions:this.chatCompletionOptions,messages:this.constructBodyWithMessages(e).messages});const t=await((s=this.engine)==null?void 0:s.chat.completions.create(this.constructBodyWithMessages(e)));if(this.debug&&console.log("[WebLlmProvider] Response:",t),t&&Symbol.asyncIterator in t)for await(const l of t){const d=(r=(o=l.choices[0])==null?void 0:o.delta)==null?void 0:r.content;d&&(yield d)}else(c=(i=(a=t==null?void 0:t.choices)==null?void 0:a[0])==null?void 0:i.message)!=null&&c.content&&(yield t.choices[0].message.content)}}class ce{constructor(e){this.model=e.model,this.stream=e.stream??!0,this.debug=e.debug??!1,this.headers={"Content-Type":"application/json",...e.headers},this.endpoint=e.baseUrl??"http://localhost:11434/api/generate"}async*sendMessages(e){const t=e.filter(r=>typeof r.content=="string").map(r=>r.content).join(`
`),s={model:this.model,prompt:t,stream:this.stream};this.debug&&console.log("[OllamaProvider] Request:",{endpoint:this.endpoint,headers:this.headers,body:s});const o=await fetch(this.endpoint,{method:"POST",headers:this.headers,body:JSON.stringify(s)});if(!o.ok)throw new Error(`Ollama API error ${o.status}: ${await o.text()}`);if(this.stream){if(!o.body)throw new Error("No response body for streaming");const r=o.body.getReader(),a=new TextDecoder;let i="";for(;;){const{value:c,done:l}=await r.read();if(l)break;i+=a.decode(c,{stream:!0});const d=i.split(`
`);i=d.pop();for(const h of d)if(h.trim())try{const u=JSON.parse(h);u.response&&(yield u.response)}catch(u){this.debug&&console.error("Ollama stream parse error:",h,u)}}}else{const r=await o.json();r.response&&(yield r.response)}}}exports.GeminiProvider=ne;exports.OllamaProvider=ce;exports.OpenaiProvider=ae;exports.WebLlmProvider=ie;exports.default=oe;
