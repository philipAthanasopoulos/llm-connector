"use strict";var _=Object.create;var $=Object.defineProperty;var z=Object.getOwnPropertyDescriptor;var G=Object.getOwnPropertyNames;var L=Object.getPrototypeOf,N=Object.prototype.hasOwnProperty;var K=(a,e,t,s)=>{if(e&&typeof e=="object"||typeof e=="function")for(let n of G(e))!N.call(a,n)&&n!==t&&$(a,n,{get:()=>e[n],enumerable:!(s=z(e,n))||s.enumerable});return a};var J=(a,e,t)=>(t=a!=null?_(L(a)):{},K(e||!a||!a.__esModule?$(t,"default",{value:a,enumerable:!0}):t,a));Object.defineProperties(exports,{__esModule:{value:!0},[Symbol.toStringTag]:{value:"Module"}});const b=require("react"),p=require("react-chatbotify"),Y={autoConfig:!0},q=(a,e)=>{const t=b.useCallback(s=>{const r=a()[s.data.nextPath];e(r)},[a,e]);p.useOnRcbEvent(p.RcbEvent.CHANGE_PATH,t)},H=(a,e)=>{const{outputTypeRef:t}=a,{toggleTextAreaDisabled:s,toggleIsBotTyping:n,focusTextArea:r,injectMessage:o,simulateStreamMessage:i,getIsChatBotVisible:c}=e,d=b.useCallback(l=>{var m;const u=l.data.block;u.llmConnector&&(l.preventDefault(),l.type==="rcb-pre-process-block"&&((m=u.llmConnector)!=null&&m.initialMessage&&(t.current==="full"?o(a.initialMessageRef.current):i(a.initialMessageRef.current)),n(!1),s(!1),setTimeout(()=>{c()&&r()})))},[n,s,r,c]);p.useOnRcbEvent(p.RcbEvent.PRE_PROCESS_BLOCK,d),p.useOnRcbEvent(p.RcbEvent.POST_PROCESS_BLOCK,d)},V=async function*(a,e){for await(const t of a)for(const s of t)yield s,await new Promise(n=>setTimeout(n,e))},Q=async function*(a,e){for await(const t of a)yield t,await new Promise(s=>setTimeout(s,e))},X=async function*(a,e,t){e==="character"?yield*V(a,t):yield*Q(a,t)},Z=async function*(a,e){for await(const t of a)e(t),yield t},ee=async(a,e,t,s={})=>{var R,M;if(!e.providerRef.current)return;const{speakAudio:n,toggleIsBotTyping:r,toggleTextAreaDisabled:o,focusTextArea:i,injectMessage:c,streamMessage:d,endStreamMessage:l,getIsChatBotVisible:u}=t,m=e.providerRef.current.sendMessages(a),f=e.outputTypeRef.current,g=e.outputSpeedRef.current;if(f==="full"){let h="";for await(const y of m){if((R=s.signal)!=null&&R.aborted)break;h+=y}r(!1),c(h),setTimeout(()=>{o(!1),u()&&i()})}else{const h=X(Z(m,n),f,g);let y="",S=!1;for await(const C of h){if((M=s.signal)!=null&&M.aborted)break;S||(r(!1),S=!0),y+=C,d(y)}l(),setTimeout(()=>{o(!1),u()&&i()})}},te=500,se=(a,e)=>{const{messagesRef:t,outputTypeRef:s,onUserMessageRef:n,onKeyDownRef:r,errorMessageRef:o}=a,{injectMessage:i,simulateStreamMessage:c,toggleTextAreaDisabled:d,toggleIsBotTyping:l,goToPath:u,focusTextArea:m,getIsChatBotVisible:f}=e,g=b.useRef(null),R=b.useCallback(M=>{if(!a.providerRef.current)return;const h=M.data.message,y=h.sender.toUpperCase();h.tags=h.tags??[],h.tags.push(`rcb-llm-connector-plugin:${y}`),y==="USER"&&(l(!0),d(!0),setTimeout(async()=>{var v;if(n.current){const P=await n.current(h);if(P)return(v=g.current)==null||v.abort(),g.current=null,u(P)}const S=a.historySizeRef.current,C=t.current,T=S?[...C.slice(-(S-1)),h]:[h],E=new AbortController;g.current=E,ee(T,a,e,{signal:E.signal}).catch(P=>{l(!1),d(!1),setTimeout(()=>{f()&&m()}),console.error("LLM prompt failed",P),s.current==="full"?i(o.current):c(o.current)})},te))},[a,e]);p.useOnRcbEvent(p.RcbEvent.POST_INJECT_MESSAGE,R),p.useOnRcbEvent(p.RcbEvent.STOP_SIMULATE_STREAM_MESSAGE,R),p.useOnRcbEvent(p.RcbEvent.STOP_STREAM_MESSAGE,R),b.useEffect(()=>{const M=async h=>{var y;if(r.current){const S=await r.current(h);S&&((y=g.current)==null||y.abort(),g.current=null,u(S))}};return window.addEventListener("keydown",M),()=>window.removeEventListener("keydown",M)},[])},re=a=>{const e=b.useRef([]),t=b.useRef(null),s=b.useRef("chunk"),n=b.useRef(30),r=b.useRef(0),o=b.useRef(""),i=b.useRef("Unable to get response, please try again."),c=b.useRef(null),d=b.useRef(null),{getFlow:l}=p.useFlow(),{speakAudio:u}=p.useAudio(),{messages:m,injectMessage:f,simulateStreamMessage:g,streamMessage:R,endStreamMessage:M}=p.useMessages(),{goToPath:h}=p.usePaths(),{toggleTextAreaDisabled:y,focusTextArea:S}=p.useTextArea(),{toggleIsBotTyping:C,getIsChatBotVisible:T}=p.useChatWindow(),E={...Y,...a??{}};b.useEffect(()=>{e.current=m},[m]),q(l,w=>{var x,A,k,B,U,F,I,W,j,D;t.current=((x=w.llmConnector)==null?void 0:x.provider)??null,s.current=((A=w.llmConnector)==null?void 0:A.outputType)??"chunk",n.current=((k=w.llmConnector)==null?void 0:k.outputSpeed)??30,r.current=((B=w.llmConnector)==null?void 0:B.historySize)??0,o.current=((U=w.llmConnector)==null?void 0:U.initialMessage)??"",i.current=((F=w.llmConnector)==null?void 0:F.errorMessage)??"Unable to get response, please try again.",c.current=((W=(I=w.llmConnector)==null?void 0:I.stopConditions)==null?void 0:W.onUserMessage)??null,d.current=((D=(j=w.llmConnector)==null?void 0:j.stopConditions)==null?void 0:D.onKeyDown)??null});const v={providerRef:t,messagesRef:e,outputTypeRef:s,outputSpeedRef:n,historySizeRef:r,initialMessageRef:o,errorMessageRef:i,onUserMessageRef:c,onKeyDownRef:d},P={speakAudio:u,injectMessage:f,simulateStreamMessage:g,streamMessage:R,endStreamMessage:M,toggleTextAreaDisabled:y,toggleIsBotTyping:C,focusTextArea:S,goToPath:h,getIsChatBotVisible:T};H(v,P),se(v,P);const O={name:"@rcb-plugins/llm-connector"};return E!=null&&E.autoConfig&&(O.settings={event:{rcbChangePath:!0,rcbPostInjectMessage:!0,rcbStopSimulateStreamMessage:!0,rcbStopStreamMessage:!0,rcbPreProcessBlock:!0,rcbPostProcessBlock:!0}}),O},oe=a=>()=>re(a);class ne{constructor(e){this.debug=!1,this.roleMap=s=>{switch(s){case"USER":return"user";default:return"model"}},this.constructBodyWithMessages=s=>{let n;return this.messageParser?n=this.messageParser(s):n=s.filter(o=>typeof o.content=="string"&&o.sender.toUpperCase()!=="SYSTEM").map(o=>{const i=this.roleMap(o.sender.toUpperCase()),c=o.content;return{role:i,parts:[{text:c}]}}),this.systemMessage&&(n=[{role:"user",parts:[{text:this.systemMessage}]},...n]),{contents:n,...this.body}},this.handleStreamResponse=async function*(s){var o,i,c,d,l;const n=new TextDecoder("utf-8");let r="";for(;;){const{value:u,done:m}=await s.read();if(m)break;r+=n.decode(u,{stream:!0});const f=r.split(`
`);r=f.pop();for(const g of f){const R=g.trim();if(!R.startsWith("data: "))continue;const M=R.slice(6);try{const y=(l=(d=(c=(i=(o=JSON.parse(M).candidates)==null?void 0:o[0])==null?void 0:i.content)==null?void 0:c.parts)==null?void 0:d[0])==null?void 0:l.text;y&&(yield y)}catch(h){console.error("SSE JSON parse error:",M,h)}}}},this.method=e.method??"POST",this.body=e.body??{},this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.debug=e.debug??!1,this.headers={"Content-Type":"application/json",Accept:this.responseFormat==="stream"?"text/event-stream":"application/json",...e.headers};const t=e.baseUrl??"https://generativelanguage.googleapis.com/v1beta";if(e.mode==="direct")this.endpoint=this.responseFormat==="stream"?`${t}/models/${e.model}:streamGenerateContent?alt=sse&key=${e.apiKey||""}`:`${t}/models/${e.model}:generateContent?key=${e.apiKey||""}`;else if(e.mode==="proxy")this.endpoint=`${t}/${e.model}`;else throw Error("Invalid mode specified for Gemini provider ('direct' or 'proxy').")}async*sendMessages(e){var s,n,r,o,i;if(this.debug){const c=this.endpoint.replace(/\?key=([^&]+)/,"?key=[REDACTED]"),d={...this.headers};console.log("[GeminiProvider] Request:",{method:this.method,endpoint:c,headers:d,body:this.constructBodyWithMessages(e)})}const t=await fetch(this.endpoint,{method:this.method,headers:this.headers,body:JSON.stringify(this.constructBodyWithMessages(e))});if(this.debug&&console.log("[GeminiProvider] Response status:",t.status),!t.ok)throw new Error(`Gemini API error ${t.status}: ${await t.text()}`);if(this.responseFormat==="stream"){if(!t.body)throw new Error("Response body is empty – cannot stream");const c=t.body.getReader();for await(const d of this.handleStreamResponse(c))yield d}else{const c=await t.json();this.debug&&console.log("[GeminiProvider] Response body:",c);const d=(i=(o=(r=(n=(s=c.candidates)==null?void 0:s[0])==null?void 0:n.content)==null?void 0:r.parts)==null?void 0:o[0])==null?void 0:i.text;if(typeof d=="string")yield d;else throw new Error("Unexpected response shape – no text candidate")}}}class ae{constructor(e){if(this.debug=!1,this.roleMap=t=>{switch(t){case"USER":return"user";case"SYSTEM":return"system";default:return"assistant"}},this.constructBodyWithMessages=t=>{let s;return this.messageParser?s=this.messageParser(t):s=t.filter(r=>typeof r.content=="string"&&r.sender.toUpperCase()!=="SYSTEM").map(r=>{const o=this.roleMap(r.sender.toUpperCase()),i=r.content;return{role:o,content:i}}),this.systemMessage&&(s=[{role:"system",content:this.systemMessage},...s]),{messages:s,...this.body}},this.handleStreamResponse=async function*(t){var r,o,i;const s=new TextDecoder("utf-8");let n="";for(;;){const{value:c,done:d}=await t.read();if(d)break;n+=s.decode(c,{stream:!0});const l=n.split(/\r?\n/);n=l.pop();for(const u of l){if(!u.startsWith("data: "))continue;const m=u.slice(6).trim();if(m==="[DONE]")return;try{const g=(i=(o=(r=JSON.parse(m).choices)==null?void 0:r[0])==null?void 0:o.delta)==null?void 0:i.content;g&&(yield g)}catch(f){console.error("Stream parse error",f)}}}},this.method=e.method??"POST",this.endpoint=e.baseUrl??"https://api.openai.com/v1/chat/completions",this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.debug=e.debug??!1,this.headers={"Content-Type":"application/json",Accept:this.responseFormat==="stream"?"text/event-stream":"application/json",...e.headers},this.body={model:e.model,stream:this.responseFormat==="stream",...e.body},e.mode==="direct"){this.headers={...this.headers,Authorization:`Bearer ${e.apiKey}`};return}if(e.mode!=="proxy")throw Error("Invalid mode specified for OpenAI provider ('direct' or 'proxy').")}async*sendMessages(e){var s,n,r;if(this.debug){const o={...this.headers};delete o.Authorization,console.log("[OpenaiProvider] Request:",{method:this.method,endpoint:this.endpoint,headers:o,body:this.constructBodyWithMessages(e)})}const t=await fetch(this.endpoint,{method:this.method,headers:this.headers,body:JSON.stringify(this.constructBodyWithMessages(e))});if(this.debug&&console.log("[OpenaiProvider] Response status:",t.status),!t.ok)throw new Error(`Openai API error ${t.status}: ${await t.text()}`);if(this.responseFormat==="stream"){if(!t.body)throw new Error("Response body is empty – cannot stream");const o=t.body.getReader();for await(const i of this.handleStreamResponse(o))yield i}else{const o=await t.json();this.debug&&console.log("[OpenaiProvider] Response body:",o);const i=(r=(n=(s=o.choices)==null?void 0:s[0])==null?void 0:n.message)==null?void 0:r.content;if(typeof i=="string")yield i;else throw new Error("Unexpected response shape – no text candidate")}}}class ie{constructor(e){this.debug=!1,this.roleMap=t=>{switch(t){case"USER":return"user";case"SYSTEM":return"system";default:return"assistant"}},this.constructBodyWithMessages=t=>{let s;return this.messageParser?s=this.messageParser(t):s=t.filter(r=>typeof r.content=="string"&&r.sender.toUpperCase()!=="SYSTEM").map(r=>{const o=this.roleMap(r.sender.toUpperCase()),i=r.content;return{role:o,content:i}}),this.systemMessage&&(s=[{role:"system",content:this.systemMessage},...s]),{messages:s,stream:this.responseFormat==="stream",...this.chatCompletionOptions}},this.model=e.model,this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.engineConfig=e.engineConfig??{},this.chatCompletionOptions=e.chatCompletionOptions??{},this.debug=e.debug??!1,this.createEngine()}async createEngine(){const{CreateMLCEngine:e}=await import("@mlc-ai/web-llm");this.engine=await e(this.model,{...this.engineConfig})}async*sendMessages(e){var s,n,r,o,i,c;this.engine||await this.createEngine(),this.debug&&console.log("[WebLlmProvider] Request:",{model:this.model,systemMessage:this.systemMessage,responseFormat:this.responseFormat,engineConfig:this.engineConfig,chatCompletionOptions:this.chatCompletionOptions,messages:this.constructBodyWithMessages(e).messages});const t=await((s=this.engine)==null?void 0:s.chat.completions.create(this.constructBodyWithMessages(e)));if(this.debug&&console.log("[WebLlmProvider] Response:",t),t&&Symbol.asyncIterator in t)for await(const d of t){const l=(r=(n=d.choices[0])==null?void 0:n.delta)==null?void 0:r.content;l&&(yield l)}else(c=(i=(o=t==null?void 0:t.choices)==null?void 0:o[0])==null?void 0:i.message)!=null&&c.content&&(yield t.choices[0].message.content)}}class ce{constructor(e){if(this.debug=!1,this.roleMap=t=>{switch(t){case"USER":return"user";case"SYSTEM":return"system";default:return"assistant"}},this.constructBodyWithMessages=t=>{let s;return this.messageParser?s=this.messageParser(t):s=t.filter(r=>typeof r.content=="string"&&r.sender.toUpperCase()!=="SYSTEM").map(r=>{const o=this.roleMap(r.sender.toUpperCase()),i=r.content;return{role:o,content:i}}),this.systemMessage&&(s=[{role:"system",content:this.systemMessage},...s]),{messages:s,...this.body}},this.handleStreamResponse=async function*(t){var r,o,i;const s=new TextDecoder("utf-8");let n="";for(;;){const{value:c,done:d}=await t.read();if(d)break;n+=s.decode(c,{stream:!0});const l=n.split(/\r?\n/);n=l.pop();for(const u of l){if(!u.startsWith("data: "))continue;const m=u.slice(6).trim();try{const f=JSON.parse(m);if(f.done===!0)return;const g=(i=(o=(r=f.choices)==null?void 0:r[0])==null?void 0:o.delta)==null?void 0:i.content;g&&(yield g)}catch(f){console.error("Stream parse error",f)}}}},this.method=e.method??"POST",this.endpoint=e.baseUrl??"https://api.openai.com/v1/chat/completions",this.systemMessage=e.systemMessage,this.responseFormat=e.responseFormat??"stream",this.messageParser=e.messageParser,this.debug=e.debug??!1,this.headers={"Content-Type":"application/json",Accept:this.responseFormat==="stream"?"text/event-stream":"application/json",...e.headers},this.body={model:e.model,stream:this.responseFormat==="stream",...e.body},e.mode==="direct"){this.headers={...this.headers,Authorization:`Bearer ${e.apiKey}`};return}if(e.mode!=="proxy")throw Error("Invalid mode specified for OpenAI provider ('direct' or 'proxy').")}async*sendMessages(e){var s,n,r;if(this.debug){const o={...this.headers};delete o.Authorization,console.log("[OpenaiProvider] Request:",{method:this.method,endpoint:this.endpoint,headers:o,body:this.constructBodyWithMessages(e)})}const t=await fetch(this.endpoint,{method:this.method,headers:this.headers,body:JSON.stringify(this.constructBodyWithMessages(e))});if(this.debug&&console.log("[OpenaiProvider] Response status:",t.status),!t.ok)throw new Error(`Openai API error ${t.status}: ${await t.text()}`);if(this.responseFormat==="stream"){if(!t.body)throw new Error("Response body is empty – cannot stream");const o=t.body.getReader();for await(const i of this.handleStreamResponse(o))yield i}else{const o=await t.json();this.debug&&console.log("[OpenaiProvider] Response body:",o);const i=(r=(n=(s=o.choices)==null?void 0:s[0])==null?void 0:n.message)==null?void 0:r.content;if(typeof i=="string")yield i;else throw new Error("Unexpected response shape – no text candidate")}}}exports.GeminiProvider=ne;exports.OllamaProvider=ce;exports.OpenaiProvider=ae;exports.WebLlmProvider=ie;exports.default=oe;
